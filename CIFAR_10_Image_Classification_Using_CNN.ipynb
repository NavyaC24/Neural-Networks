{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR-10 Image Classification Using CNN",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NavyaC24/Neural-Networks/blob/master/CIFAR_10_Image_Classification_Using_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "HHthCws9b1vU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h4LlKjCFcUHs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5321
        },
        "outputId": "f7fc1934-5271-4ca3-f213-acf367f68ca0"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.datasets import cifar10\n",
        "from keras import regularizers\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import numpy as np\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    lrate = 0.001\n",
        "    if epoch > 75:\n",
        "        lrate = 0.0005\n",
        "    elif epoch > 100:\n",
        "        lrate = 0.0003        \n",
        "    return lrate\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "#z-score\n",
        "mean = np.mean(x_train,axis=(0,1,2,3))\n",
        "std = np.std(x_train,axis=(0,1,2,3))\n",
        "x_train = (x_train-mean)/(std+1e-7)\n",
        "x_test = (x_test-mean)/(std+1e-7)\n",
        "\n",
        "num_classes = 10\n",
        "y_train = np_utils.to_categorical(y_train,num_classes)\n",
        "y_test = np_utils.to_categorical(y_test,num_classes)\n",
        "\n",
        "weight_decay = 1e-4\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    )\n",
        "datagen.fit(x_train)\n",
        "\n",
        "#training\n",
        "batch_size = 64\n",
        "\n",
        "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
        "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=125,\\\n",
        "                    verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])\n",
        "\n",
        "#save to disk\n",
        "model_json = model.to_json()\n",
        "with open('model.json', 'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "model.save_weights('model.h5')    \n",
        "\n",
        "#testing\n",
        "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
        "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_13 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 309,290\n",
            "Trainable params: 308,394\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "Epoch 1/125\n",
            "781/781 [==============================] - 48s 62ms/step - loss: 1.8820 - acc: 0.4325 - val_loss: 1.4282 - val_acc: 0.5657\n",
            "Epoch 2/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 1.2734 - acc: 0.5946 - val_loss: 1.1811 - val_acc: 0.6481\n",
            "Epoch 3/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 1.0725 - acc: 0.6571 - val_loss: 1.1933 - val_acc: 0.6608\n",
            "Epoch 4/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.9753 - acc: 0.6941 - val_loss: 0.8643 - val_acc: 0.7416\n",
            "Epoch 5/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.9126 - acc: 0.7180 - val_loss: 0.9870 - val_acc: 0.7180\n",
            "Epoch 6/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.8665 - acc: 0.7347 - val_loss: 0.8162 - val_acc: 0.7660\n",
            "Epoch 7/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.8332 - acc: 0.7500 - val_loss: 0.8357 - val_acc: 0.7591\n",
            "Epoch 8/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.8077 - acc: 0.7586 - val_loss: 0.7626 - val_acc: 0.7800\n",
            "Epoch 9/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.7828 - acc: 0.7695 - val_loss: 0.7481 - val_acc: 0.7878\n",
            "Epoch 10/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.7611 - acc: 0.7778 - val_loss: 0.6906 - val_acc: 0.8111\n",
            "Epoch 11/125\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 0.7486 - acc: 0.7813 - val_loss: 0.6488 - val_acc: 0.8233\n",
            "Epoch 12/125\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 0.7384 - acc: 0.7883 - val_loss: 0.7152 - val_acc: 0.8097\n",
            "Epoch 13/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.7243 - acc: 0.7939 - val_loss: 0.7264 - val_acc: 0.7986\n",
            "Epoch 14/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.7140 - acc: 0.7963 - val_loss: 0.8380 - val_acc: 0.7756\n",
            "Epoch 15/125\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 0.7028 - acc: 0.8019 - val_loss: 0.7377 - val_acc: 0.7934\n",
            "Epoch 16/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.6940 - acc: 0.8062 - val_loss: 0.7235 - val_acc: 0.8033\n",
            "Epoch 17/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.6835 - acc: 0.8087 - val_loss: 0.6835 - val_acc: 0.8168\n",
            "Epoch 18/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.6845 - acc: 0.8099 - val_loss: 0.7210 - val_acc: 0.8055\n",
            "Epoch 19/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.6770 - acc: 0.8134 - val_loss: 0.6015 - val_acc: 0.8449\n",
            "Epoch 20/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.6698 - acc: 0.8148 - val_loss: 0.6344 - val_acc: 0.8334\n",
            "Epoch 21/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.6657 - acc: 0.8158 - val_loss: 0.6239 - val_acc: 0.8369\n",
            "Epoch 22/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.6582 - acc: 0.8200 - val_loss: 0.7075 - val_acc: 0.8087\n",
            "Epoch 23/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.6527 - acc: 0.8222 - val_loss: 0.6897 - val_acc: 0.8284\n",
            "Epoch 24/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.6540 - acc: 0.8228 - val_loss: 0.6381 - val_acc: 0.8369\n",
            "Epoch 25/125\n",
            "781/781 [==============================] - 46s 58ms/step - loss: 0.6499 - acc: 0.8242 - val_loss: 0.6131 - val_acc: 0.8432\n",
            "Epoch 26/125\n",
            "781/781 [==============================] - 46s 60ms/step - loss: 0.6445 - acc: 0.8274 - val_loss: 0.6158 - val_acc: 0.8393\n",
            "Epoch 27/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.6371 - acc: 0.8299 - val_loss: 0.6158 - val_acc: 0.8410\n",
            "Epoch 28/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.6404 - acc: 0.8272 - val_loss: 0.6543 - val_acc: 0.8342\n",
            "Epoch 29/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.6378 - acc: 0.8283 - val_loss: 0.6612 - val_acc: 0.8256\n",
            "Epoch 30/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.6311 - acc: 0.8310 - val_loss: 0.5698 - val_acc: 0.8551\n",
            "Epoch 31/125\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 0.6291 - acc: 0.8323 - val_loss: 0.5994 - val_acc: 0.8485\n",
            "Epoch 32/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.6248 - acc: 0.8346 - val_loss: 0.6053 - val_acc: 0.8474\n",
            "Epoch 33/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.6262 - acc: 0.8335 - val_loss: 0.6768 - val_acc: 0.8288\n",
            "Epoch 34/125\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 0.6237 - acc: 0.8338 - val_loss: 0.6065 - val_acc: 0.8440\n",
            "Epoch 35/125\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 0.6239 - acc: 0.8334 - val_loss: 0.6049 - val_acc: 0.8501\n",
            "Epoch 36/125\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 0.6174 - acc: 0.8379 - val_loss: 0.5935 - val_acc: 0.8497\n",
            "Epoch 37/125\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 0.6176 - acc: 0.8385 - val_loss: 0.6047 - val_acc: 0.8524\n",
            "Epoch 38/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.6138 - acc: 0.8352 - val_loss: 0.6067 - val_acc: 0.8501\n",
            "Epoch 39/125\n",
            "781/781 [==============================] - 47s 61ms/step - loss: 0.6120 - acc: 0.8402 - val_loss: 0.6437 - val_acc: 0.8427\n",
            "Epoch 40/125\n",
            "781/781 [==============================] - 46s 58ms/step - loss: 0.6089 - acc: 0.8421 - val_loss: 0.5529 - val_acc: 0.8672\n",
            "Epoch 41/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.6117 - acc: 0.8421 - val_loss: 0.6088 - val_acc: 0.8489\n",
            "Epoch 42/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.6113 - acc: 0.8392 - val_loss: 0.6385 - val_acc: 0.8392\n",
            "Epoch 43/125\n",
            "781/781 [==============================] - 46s 58ms/step - loss: 0.6066 - acc: 0.8437 - val_loss: 0.5679 - val_acc: 0.8599\n",
            "Epoch 44/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.6109 - acc: 0.8407 - val_loss: 0.6443 - val_acc: 0.8393\n",
            "Epoch 45/125\n",
            "781/781 [==============================] - 46s 58ms/step - loss: 0.6021 - acc: 0.8448 - val_loss: 0.6674 - val_acc: 0.8298\n",
            "Epoch 46/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.6033 - acc: 0.8447 - val_loss: 0.5834 - val_acc: 0.8583\n",
            "Epoch 47/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.5999 - acc: 0.8437 - val_loss: 0.7009 - val_acc: 0.8173\n",
            "Epoch 48/125\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 0.5980 - acc: 0.8456 - val_loss: 0.6119 - val_acc: 0.8486\n",
            "Epoch 49/125\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 0.6030 - acc: 0.8435 - val_loss: 0.5917 - val_acc: 0.8577\n",
            "Epoch 50/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.5996 - acc: 0.8466 - val_loss: 0.5891 - val_acc: 0.8536\n",
            "Epoch 51/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.5944 - acc: 0.8459 - val_loss: 0.5552 - val_acc: 0.8651\n",
            "Epoch 52/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.5955 - acc: 0.8461 - val_loss: 0.5833 - val_acc: 0.8581\n",
            "Epoch 53/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.5964 - acc: 0.8478 - val_loss: 0.5857 - val_acc: 0.8561\n",
            "Epoch 54/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.5901 - acc: 0.8501 - val_loss: 0.5895 - val_acc: 0.8542\n",
            "Epoch 55/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.5963 - acc: 0.8461 - val_loss: 0.5955 - val_acc: 0.8530\n",
            "Epoch 56/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.5899 - acc: 0.8489 - val_loss: 0.6035 - val_acc: 0.8491\n",
            "Epoch 57/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.5856 - acc: 0.8502 - val_loss: 0.6340 - val_acc: 0.8426\n",
            "Epoch 58/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.5900 - acc: 0.8490 - val_loss: 0.6140 - val_acc: 0.8513\n",
            "Epoch 59/125\n",
            "781/781 [==============================] - 47s 61ms/step - loss: 0.5867 - acc: 0.8505 - val_loss: 0.6857 - val_acc: 0.8333\n",
            "Epoch 60/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.5856 - acc: 0.8497 - val_loss: 0.5915 - val_acc: 0.8559\n",
            "Epoch 61/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.5848 - acc: 0.8499 - val_loss: 0.6925 - val_acc: 0.8263\n",
            "Epoch 62/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.5871 - acc: 0.8503 - val_loss: 0.5924 - val_acc: 0.8523\n",
            "Epoch 63/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.5907 - acc: 0.8502 - val_loss: 0.6816 - val_acc: 0.8293\n",
            "Epoch 64/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.5884 - acc: 0.8500 - val_loss: 0.6301 - val_acc: 0.8425\n",
            "Epoch 65/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.5799 - acc: 0.8521 - val_loss: 0.5865 - val_acc: 0.8565\n",
            "Epoch 66/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.5891 - acc: 0.8494 - val_loss: 0.5960 - val_acc: 0.8528\n",
            "Epoch 67/125\n",
            "781/781 [==============================] - 45s 58ms/step - loss: 0.5797 - acc: 0.8517 - val_loss: 0.6084 - val_acc: 0.8514\n",
            "Epoch 68/125\n",
            "781/781 [==============================] - 46s 58ms/step - loss: 0.5779 - acc: 0.8541 - val_loss: 0.5528 - val_acc: 0.8659\n",
            "Epoch 69/125\n",
            "781/781 [==============================] - 46s 58ms/step - loss: 0.5830 - acc: 0.8530 - val_loss: 0.5824 - val_acc: 0.8607\n",
            "Epoch 70/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.5799 - acc: 0.8517 - val_loss: 0.5871 - val_acc: 0.8585\n",
            "Epoch 71/125\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 0.5768 - acc: 0.8536 - val_loss: 0.5709 - val_acc: 0.8617\n",
            "Epoch 72/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.5770 - acc: 0.8520 - val_loss: 0.6029 - val_acc: 0.8564\n",
            "Epoch 73/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.5753 - acc: 0.8549 - val_loss: 0.6513 - val_acc: 0.8386\n",
            "Epoch 74/125\n",
            "781/781 [==============================] - 45s 58ms/step - loss: 0.5752 - acc: 0.8536 - val_loss: 0.6227 - val_acc: 0.8472\n",
            "Epoch 75/125\n",
            "781/781 [==============================] - 45s 58ms/step - loss: 0.5773 - acc: 0.8541 - val_loss: 0.5449 - val_acc: 0.8658\n",
            "Epoch 76/125\n",
            "781/781 [==============================] - 45s 58ms/step - loss: 0.5729 - acc: 0.8541 - val_loss: 0.5647 - val_acc: 0.8666\n",
            "Epoch 77/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.5374 - acc: 0.8673 - val_loss: 0.5335 - val_acc: 0.8725\n",
            "Epoch 78/125\n",
            "781/781 [==============================] - 45s 58ms/step - loss: 0.5249 - acc: 0.8704 - val_loss: 0.5111 - val_acc: 0.8790\n",
            "Epoch 79/125\n",
            "781/781 [==============================] - 45s 58ms/step - loss: 0.5081 - acc: 0.8746 - val_loss: 0.5482 - val_acc: 0.8667\n",
            "Epoch 80/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.5036 - acc: 0.8749 - val_loss: 0.5583 - val_acc: 0.8644\n",
            "Epoch 81/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.4982 - acc: 0.8762 - val_loss: 0.5193 - val_acc: 0.8745\n",
            "Epoch 82/125\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 0.4970 - acc: 0.8757 - val_loss: 0.5320 - val_acc: 0.8737\n",
            "Epoch 83/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.4852 - acc: 0.8787 - val_loss: 0.4950 - val_acc: 0.8806\n",
            "Epoch 84/125\n",
            "781/781 [==============================] - 45s 58ms/step - loss: 0.4894 - acc: 0.8743 - val_loss: 0.5076 - val_acc: 0.8781\n",
            "Epoch 85/125\n",
            "781/781 [==============================] - 45s 58ms/step - loss: 0.4789 - acc: 0.8803 - val_loss: 0.5432 - val_acc: 0.8677\n",
            "Epoch 86/125\n",
            "781/781 [==============================] - 45s 58ms/step - loss: 0.4769 - acc: 0.8770 - val_loss: 0.5160 - val_acc: 0.8748\n",
            "Epoch 87/125\n",
            "781/781 [==============================] - 45s 58ms/step - loss: 0.4758 - acc: 0.8803 - val_loss: 0.5080 - val_acc: 0.8738\n",
            "Epoch 88/125\n",
            "781/781 [==============================] - 47s 60ms/step - loss: 0.4815 - acc: 0.8779 - val_loss: 0.4752 - val_acc: 0.8834\n",
            "Epoch 89/125\n",
            "781/781 [==============================] - 46s 58ms/step - loss: 0.4708 - acc: 0.8798 - val_loss: 0.5015 - val_acc: 0.8787\n",
            "Epoch 90/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.4764 - acc: 0.8785 - val_loss: 0.5124 - val_acc: 0.8728\n",
            "Epoch 91/125\n",
            "781/781 [==============================] - 45s 58ms/step - loss: 0.4636 - acc: 0.8801 - val_loss: 0.5036 - val_acc: 0.8760\n",
            "Epoch 92/125\n",
            "781/781 [==============================] - 45s 58ms/step - loss: 0.4650 - acc: 0.8816 - val_loss: 0.4949 - val_acc: 0.8789\n",
            "Epoch 93/125\n",
            "781/781 [==============================] - 46s 58ms/step - loss: 0.4662 - acc: 0.8802 - val_loss: 0.4845 - val_acc: 0.8799\n",
            "Epoch 94/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.4687 - acc: 0.8798 - val_loss: 0.4579 - val_acc: 0.8872\n",
            "Epoch 95/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.4638 - acc: 0.8818 - val_loss: 0.4859 - val_acc: 0.8787\n",
            "Epoch 96/125\n",
            "781/781 [==============================] - 45s 58ms/step - loss: 0.4612 - acc: 0.8814 - val_loss: 0.4955 - val_acc: 0.8795\n",
            "Epoch 97/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.4627 - acc: 0.8787 - val_loss: 0.5404 - val_acc: 0.8630\n",
            "Epoch 98/125\n",
            "781/781 [==============================] - 46s 59ms/step - loss: 0.4583 - acc: 0.8800 - val_loss: 0.4988 - val_acc: 0.8739\n",
            "Epoch 99/125\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 0.4601 - acc: 0.8806 - val_loss: 0.5056 - val_acc: 0.8732\n",
            "Epoch 100/125\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 0.4577 - acc: 0.8800 - val_loss: 0.4768 - val_acc: 0.8857\n",
            "Epoch 101/125\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 0.4574 - acc: 0.8814 - val_loss: 0.5162 - val_acc: 0.8681\n",
            "Epoch 102/125\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 0.4595 - acc: 0.8813 - val_loss: 0.5000 - val_acc: 0.8743\n",
            "Epoch 103/125\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 0.4596 - acc: 0.8801 - val_loss: 0.5054 - val_acc: 0.8752\n",
            "Epoch 104/125\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 0.4593 - acc: 0.8796 - val_loss: 0.4901 - val_acc: 0.8777\n",
            "Epoch 105/125\n",
            "781/781 [==============================] - 50s 63ms/step - loss: 0.4508 - acc: 0.8827 - val_loss: 0.4823 - val_acc: 0.8795\n",
            "Epoch 106/125\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 0.4557 - acc: 0.8818 - val_loss: 0.4814 - val_acc: 0.8780\n",
            "Epoch 107/125\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 0.4518 - acc: 0.8839 - val_loss: 0.5001 - val_acc: 0.8747\n",
            "Epoch 108/125\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 0.4553 - acc: 0.8816 - val_loss: 0.4718 - val_acc: 0.8848\n",
            "Epoch 109/125\n",
            "781/781 [==============================] - 49s 63ms/step - loss: 0.4571 - acc: 0.8806 - val_loss: 0.4873 - val_acc: 0.8783\n",
            "Epoch 110/125\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 0.4477 - acc: 0.8829 - val_loss: 0.4814 - val_acc: 0.8819\n",
            "Epoch 111/125\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 0.4499 - acc: 0.8822 - val_loss: 0.4543 - val_acc: 0.8881\n",
            "Epoch 112/125\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 0.4509 - acc: 0.8822 - val_loss: 0.4795 - val_acc: 0.8797\n",
            "Epoch 113/125\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 0.4473 - acc: 0.8834 - val_loss: 0.4960 - val_acc: 0.8740\n",
            "Epoch 114/125\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 0.4451 - acc: 0.8849 - val_loss: 0.5031 - val_acc: 0.8747\n",
            "Epoch 115/125\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 0.4515 - acc: 0.8814 - val_loss: 0.4948 - val_acc: 0.8738\n",
            "Epoch 116/125\n",
            "781/781 [==============================] - 50s 63ms/step - loss: 0.4474 - acc: 0.8824 - val_loss: 0.5295 - val_acc: 0.8662\n",
            "Epoch 117/125\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 0.4455 - acc: 0.8828 - val_loss: 0.4966 - val_acc: 0.8753\n",
            "Epoch 118/125\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 0.4464 - acc: 0.8840 - val_loss: 0.4817 - val_acc: 0.8791\n",
            "Epoch 119/125\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 0.4453 - acc: 0.8857 - val_loss: 0.4937 - val_acc: 0.8785\n",
            "Epoch 120/125\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 0.4521 - acc: 0.8821 - val_loss: 0.4971 - val_acc: 0.8713\n",
            "Epoch 121/125\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 0.4421 - acc: 0.8853 - val_loss: 0.4498 - val_acc: 0.8864\n",
            "Epoch 122/125\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 0.4458 - acc: 0.8834 - val_loss: 0.4903 - val_acc: 0.8759\n",
            "Epoch 123/125\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 0.4406 - acc: 0.8852 - val_loss: 0.4858 - val_acc: 0.8777\n",
            "Epoch 124/125\n",
            "781/781 [==============================] - 50s 63ms/step - loss: 0.4427 - acc: 0.8866 - val_loss: 0.4905 - val_acc: 0.8761\n",
            "Epoch 125/125\n",
            "781/781 [==============================] - 50s 64ms/step - loss: 0.4479 - acc: 0.8826 - val_loss: 0.4633 - val_acc: 0.8869\n",
            "10000/10000 [==============================] - 2s 162us/step\n",
            "\n",
            "Test result: 88.690 loss: 0.463\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Aqiqs8Kujy3j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}